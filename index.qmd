---
title: "Credit Risk Modeling"
subtitle: "A Fast Introduction for Data Scientists — By Duk Pagnarith"
author: ""
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    transition: fade
    fontsize: 1.5em
    margin: 0.1
    scrollable: true
    title-slide-attributes:
      data-background-color: "#1a2744"
---

## What is Credit Risk?

Banks lend money. Sometimes people don't pay it back. **That's credit risk.**

- Credit risk modeling = **predicting who won't pay back and how much the bank could lose**
- It's classification + regression — but wrapped in **heavy regulations**

We predict **three things**:

| Component | Prediction | You'd Call It... |
|-----------|-----------|-----------------|
| **PD** | Will they default? | Binary classification |
| **LGD** | How much do we lose? | Regression (0–1) |
| **EAD** | How much is at risk? | Regression ($) |

**Expected Loss = PD × LGD × EAD**

---

## How Defaults Work & Target Variables

**The default timeline:**

1. **Delinquency** → missed payments (can recover)
2. **Default** → 90–180 days unpaid (officially "bad")
3. **Loss** → debt written off or collateral seized

**Building your labels:**

- **PD target:** Snapshot each borrower → did they default within 12 months? Stack multiple years for economic diversity.
- **LGD target:** (Balance − recovered + costs) ÷ balance at default. Requires years of recovery data.
- **EAD target:** Outstanding balance + fees + accrued interest at default.

> Different products (mortgage vs. credit card) and client types (retail vs. wholesale) need **separate models** — the risk drivers are completely different.

---

## What Makes Credit Risk Different from Regular ML

Things your DS instincts won't prepare you for:

- **Regulations shape everything:** Basel Accords (capital), IFRS 9 (forward-looking losses), Stress Testing
- **Explainability > accuracy:** Regulators can reject a model they can't understand
- **Feature restrictions:** No social media, browsing history, or "creative" data. Every feature needs a **business reason**
- **Conservatism is required:** PD floors, downturn weighting, margins of conservatism — models intentionally overestimate risk
- **Documentation is mandatory:** Every decision must be written down and justified for auditors

> You can't just build the best model — it also has to **follow the rules**.

---

## Data & Feature Engineering

**What you CAN use:**

- Payment history, balances, transaction patterns
- Credit bureau scores, income, debt ratios
- Economic indicators (GDP, unemployment, interest rates)

**Key features in credit risk:**

| Category | Examples |
|----------|---------|
| **Payment behavior** | Missed payments, payment amounts |
| **Delinquency** | Frequency, severity, recency |
| **Financial health** | Income, debt-to-income ratio |
| **Collateral** | Asset value securing the loan |
| **Macro** | Unemployment rate, GDP growth |

> **The rule:** If a feature boosts AUC but you can't explain *why* it predicts default → **drop it**.

---

## Point-in-Time vs. Through-the-Cycle

Two philosophies for measuring risk — picking the wrong one breaks trust:

| | Point-in-Time (PIT) | Through-the-Cycle (TTC) |
|--|---------------------|------------------------|
| **Reacts to** | Current conditions | Long-term averages |
| **Changes** | Quickly | Slowly |
| **Used for** | Loan approvals, pricing | Regulatory capital |
| **Analogy** | Today's weather | Average climate |

- **PIT** = sensitive, responsive, volatile
- **TTC** = stable, conservative, slow to react
- Many models blend both approaches depending on the use case

---

## Modeling Approaches

**The workhorses (regulators love these):**

- **Logistic Regression** → PD. Transparent, standard, trusted.
- **Scorecards** → Binned features + point weights. Dominant in retail lending.
- **Linear Regression** → LGD/EAD. Simple and explainable.

**Challenger models (you'll be asked to build these too):**

- Gradient Boosting, Random Forest — great benchmarks
- Survival models — time-to-default prediction
- Bayesian methods — when data is scarce

**Feature selection with a credit twist:**

- **IV & WoE** (Information Value / Weight of Evidence) — the standard in credit scoring
- L1/L2 regularization, tree-based importance — you know these
- But again: if you can't explain *why* → drop it

---

## Model Evaluation — What Validators Care About

| Dimension | Question | Metrics |
|-----------|----------|---------|
| **Discrimination** | Can it separate good vs. bad? | AUC, Gini |
| **Calibration** | Are predicted probabilities accurate? | Brier Score, MSE |
| **Stability** | Does it hold up over time? | PSI (Population Stability Index) |
| **Interpretability** | Can you explain it? | Qualitative |

**Beyond train/test split:**

- **Backtesting** — predictions vs. actuals over historical periods
- **Stress testing** — extreme scenarios (recession, rate spikes)
- **Sensitivity analysis** — tweak inputs, check if outputs stay reasonable

> **A model that's understood beats a model that's slightly better but opaque.**

---

## Expected vs. Unexpected Losses

**Expected Losses (EL)** — the "cost of doing business"

- Banks set aside **provisions** to cover these
- Your model needs to be **accurate**

**Unexpected Losses (UL)** — what happens during crises

- Banks hold **capital reserves** for these
- Your model needs to be **conservative** (overestimate risk)

**Conservatism mechanisms:**

- **PD floors** → never assume zero risk
- **Downturn weighting** → give more weight to recession data
- **Margins of conservatism** → regulatory buffers on top of predictions
- **Stress overlays** → worst-case scenario adjustments

---

## The Model Lifecycle

A credit risk project takes **3 months to 1 year**:

1. **Requirements** — Business need + regulatory context
2. **Data** — Extract, clean, prepare (~70% of the time, as usual)
3. **Modeling** — Build, evaluate, compare
4. **Documentation** — Everything. Why this model, this data, these features, these alternatives.
5. **Validation** — An **independent team** challenges your model
6. **Deployment & Monitoring** — Ship it, watch for drift

**Validation outcomes:**

- ✅ Approved
- ⚠️ Conditionally approved (fix issues first)
- ❌ Rejected (back to square one)

> Validation isn't a test — it's a **challenge process**. How you respond to findings matters.

---

## Key Takeaways

1. Credit risk = predicting **PD, LGD, EAD** → Expected Loss
2. **Regulations** shape everything — explainability and defensibility are non-negotiable
3. **Logistic regression** is king; ML models serve as challengers
4. Every feature needs a **business reason**, not just statistical significance
5. **Documentation and validation** are as important as the model itself
6. **Conservatism** is built in — better to overestimate risk than underestimate

> In credit risk, being right isn't enough — you need to be **understood, validated, and defensible**.
